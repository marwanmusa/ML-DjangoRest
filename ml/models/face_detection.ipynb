{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import library\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# headshots_folder_path = \"data/headshots\"\n",
    "\n",
    "# # image dimensions\n",
    "# image_width = 224\n",
    "# image_height = 224\n",
    "\n",
    "# # for detecting faces\n",
    "# facecascade = cv2.CascadeClassifier('cascade_frontalface.xml')\n",
    "\n",
    "# # set the directory containing the images\n",
    "# images_dir = os.path.join(\"..\", headshots_folder_path)\n",
    "\n",
    "# cur_id = 0\n",
    "# label_ids = {}\n",
    "\n",
    "# # iterates through all the files in each subdirectories\n",
    "# for root, _, files in os.walk(images_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('png') or\\\n",
    "#         file.endswith('jpg') or\\\n",
    "#         file.endswith('jpeg'):\n",
    "#             # path of the  image\n",
    "#             path = os.path.join(root, file)\n",
    "\n",
    "#         # get the label name (name of the person)\n",
    "#         label = os.path.basename(root).replace(\" \", \"..\").lower()\n",
    "\n",
    "#         # add the label in label_ids:\n",
    "#         label_ids[label] = cur_id\n",
    "#         cur_id += 1\n",
    "\n",
    "#         # load the image\n",
    "#         imgtest = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "#         image_array = np.array(imgtest, \"uint8\")\n",
    "\n",
    "#         # get the faces detected in the image\n",
    "#         faces = facecascade.detectMultiScale(imgtest, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "#         # if not exactly 1 face is detected, skip this photo\n",
    "#         if len(faces) != 1:\n",
    "#             print(f'--Photo Skipped--\\n')\n",
    "\n",
    "#         # remove the original image\n",
    "#         os.remove(path)\n",
    "#         continue\n",
    "\n",
    "#         for (x_, y_, w, h) in faces:\n",
    "\n",
    "#             # draw the face detected\n",
    "#             face_detect = cv2.rectangle(imgtest,\n",
    "#                                         (x_, y_),\n",
    "#                                         (x_+w, y_+h),\n",
    "#                                         (255, 0, 255), 2)\n",
    "#             plt.imshow(face_detect)\n",
    "#             plt.show()\n",
    "\n",
    "#             # resize the detected face to 224x224\n",
    "#             size = (image_width, image_height)\n",
    "\n",
    "#             # detected face region\n",
    "#             roi = image_array[y_: y_ + h, x_: x_ + w]\n",
    "\n",
    "#             # resize the detected head to target size\n",
    "#             resized_image = cv2.resize(roi, size)\n",
    "#             image_array = np.array(resized_image, \"uint8\")\n",
    "\n",
    "#             # remove the original image\n",
    "#             os.remove(path)\n",
    "\n",
    "#             # replace the image with only the face\n",
    "#             im = Image.fromarray(image_array)\n",
    "#             im.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.7.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import cv2\n",
    "from cv2 import CascadeClassifier, imread\n",
    "\n",
    "# check cv2 version\n",
    "print(\"OpenCV version: \" + cv2.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Detect Faces for Face Recognition\n",
    "Before we can perform face recognition, we need to detect faces.\n",
    "\n",
    "`Face detection` is the process of automatically locating faces in a photograph and localizing them by drawing a bounding box around their extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "14/14 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "# load image from file\n",
    "img = plt.imread(\"headshots/Marwan Musa/1.jpg\")\n",
    "\n",
    "# create the detector, using default weights\n",
    "detector = MTCNN()\n",
    "\n",
    "# detect faces in the image\n",
    "res = detector.detect_faces(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the bounding box from the first face\n",
    "x1, y1, width, height = res[0]['box']\n",
    "x2, y2 = x1+width, y1+height\n",
    "\n",
    "# extract the face\n",
    "face = img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pixels to the model size\n",
    "image = Image.fromarray(face)\n",
    "image = image.resize((224, 224))\n",
    "face_array = asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(face_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unite all the code above in a function extract_face()\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename : str, required_size : tuple = (224, 224)) -> numpy.array:\n",
    "    # load img from file\n",
    "    img = plt.imread(filename)\n",
    "\n",
    "    # create detector using default weigths\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # detect faces in the image\n",
    "    res = detector.detect_faces(img)\n",
    "\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = res[0]['box']\n",
    "    x2, y2 = x1+width, y1+height\n",
    "\n",
    "    # extract the face\n",
    "    face = img[y1:y2, x1:x2]\n",
    "\n",
    "    # resize image to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the photo and extract the face\n",
    "img = extract_face(\"headshots/Kevin Hart/3.jpg\")\n",
    "# plot the extracted face\n",
    "plt.imshow(img)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces and calculate face embeddings for a list of photo files\n",
    "def get_embeddings(filenames):\n",
    "    #extract faces\n",
    "    faces = [extract_face(filename) for filename in filenames]\n",
    "    # convert into an array of samples\n",
    "    samples = asarray(faces, 'float32')\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    samples = preprocess_input(samples, version=2)\n",
    "    # create a vggface model\n",
    "    model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "    # perform prediction\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat\n",
    "\n",
    "# determine if a candidate face is a match for a know face\n",
    "def is_match(known_embedding, candidate_embedding, thresh=0.5):\n",
    "    # calculate distance between embeddings\n",
    "    score = cosine(known_embedding, candidate_embedding)\n",
    "    if score <= thresh:\n",
    "        print(\">face is a Match (%.3f <= %.3f)\" % (score, thresh))\n",
    "    else:\n",
    "        print(\">face is NOT a Match (%.3f > %.3f)\" % (score, thresh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filenames\n",
    "filenames = [\n",
    "    \"headshots/Marwan Musa/1.jpg\",\n",
    "    \"headshots/Marwan Musa/2.jpg\",\n",
    "    \"headshots/Marwan Musa/3.jpg\",\n",
    "    \"headshots/Kevin Hart/1.jpg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "14/14 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "28/28 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "15/15 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94694792/94694792 [==============================] - 16s 0us/step\n",
      "1/1 [==============================] - 1s 845ms/step\n",
      "Positive Tests\n",
      ">face is a Match (0.290 <= 0.500)\n",
      ">face is a Match (0.252 <= 0.500)\n",
      "Negative Tests\n",
      ">face is NOT a Match (0.791 > 0.500)\n"
     ]
    }
   ],
   "source": [
    "# get embeddings file filenames\n",
    "embeddings = get_embeddings(filenames)\n",
    "# define my face\n",
    "myface_id = embeddings[0]\n",
    "# verify known photos of me\n",
    "print('Positive Tests')\n",
    "is_match(myface_id, embeddings[1])\n",
    "is_match(myface_id, embeddings[2])\n",
    "# verify known photos of other people\n",
    "print('Negative Tests')\n",
    "is_match(myface_id, embeddings[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c20545a1d769310691a62ac94296288e478b66574fc3af5140b411fe9716731"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
